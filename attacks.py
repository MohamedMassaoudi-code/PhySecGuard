"""
attacks.py
----------
This module provides adversarial attack implementations for evaluating model vulnerabilities.
It includes:
  - fgsm_attack: Fast Gradient Sign Method attack.
  - pgd_attack: Projected Gradient Descent attack.
  - cw_attack: Simplified Carlini & Wagner L2 attack.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F

def fgsm_attack(model, loss_fn, data, target, epsilon):
    """
    Fast Gradient Sign Method (FGSM) attack.
    
    Parameters:
        model (torch.nn.Module): The model to attack.
        loss_fn (torch.nn.modules.loss._Loss): Loss function used to compute gradients.
        data (torch.Tensor): Input images to be perturbed.
        target (torch.Tensor): True labels corresponding to the input data.
        epsilon (float): Perturbation magnitude.
    
    Returns:
        torch.Tensor: Adversarial examples generated by the FGSM attack.
    """
    # Clone the data and enable gradient computation
    data_clone = data.clone().detach().requires_grad_(True)
    
    # Forward pass through the model
    output = model(data_clone)
    
    # Compute loss with scalar reduction
    original_reduction = loss_fn.reduction if hasattr(loss_fn, 'reduction') else None
    if original_reduction is not None:
        loss_fn.reduction = 'mean'
    loss = loss_fn(output, target)
    
    # Compute gradients with respect to the input data
    grad = torch.autograd.grad(loss, data_clone, create_graph=False)[0]
    
    # Restore the original reduction (if applicable)
    if original_reduction is not None:
        loss_fn.reduction = original_reduction
    
    # Create adversarial examples by adding the sign of the gradients scaled by epsilon
    perturbed_data = data + epsilon * grad.sign()
    # Ensure pixel values remain within [0, 1]
    perturbed_data = torch.clamp(perturbed_data, 0, 1)
    
    return perturbed_data

def pgd_attack(model, loss_fn, data, target, epsilon, alpha=0.01, iterations=10):
    """
    Projected Gradient Descent (PGD) attack.
    
    Parameters:
        model (torch.nn.Module): The model to attack.
        loss_fn (torch.nn.modules.loss._Loss): Loss function used to compute gradients.
        data (torch.Tensor): Input images to be perturbed.
        target (torch.Tensor): True labels corresponding to the input data.
        epsilon (float): Maximum perturbation magnitude.
        alpha (float): Step size for each iteration.
        iterations (int): Number of attack iterations.
    
    Returns:
        torch.Tensor: Adversarial examples generated by the PGD attack.
    """
    data_clone = data.clone().detach()
    perturbed_data = data.clone().detach()
    
    # Store original reduction setting if applicable
    original_reduction = loss_fn.reduction if hasattr(loss_fn, 'reduction') else None
    if original_reduction is not None:
        loss_fn.reduction = 'mean'
    
    for i in range(iterations):
        perturbed_data.requires_grad_(True)
        output = model(perturbed_data)
        loss = loss_fn(output, target)
        grad = torch.autograd.grad(loss, perturbed_data, create_graph=False)[0]
        
        with torch.no_grad():
            adv_data = perturbed_data.detach() + alpha * grad.sign()
            # Project perturbation back onto the epsilon ball
            eta = torch.clamp(adv_data - data_clone, -epsilon, epsilon)
            perturbed_data = torch.clamp(data_clone + eta, 0, 1).detach()
    
    # Restore original reduction if needed
    if original_reduction is not None:
        loss_fn.reduction = original_reduction
    
    return perturbed_data

def cw_attack(model, data, target, epsilon=0.1, confidence=0, c=1, iterations=100, lr=0.01):
    """
    Simplified Carlini & Wagner (CW) L2 attack.
    
    Parameters:
        model (torch.nn.Module): The model to attack.
        data (torch.Tensor): Input images to be perturbed.
        target (torch.Tensor): True labels corresponding to the input data.
        epsilon (float): Maximum allowed perturbation magnitude.
        confidence (float): Confidence parameter for attack success.
        c (float): Weight for balancing attack and distance loss.
        iterations (int): Number of optimization iterations.
        lr (float): Learning rate for optimization.
    
    Returns:
        torch.Tensor: Adversarial examples generated by the CW attack.
    """
    data_clone = data.clone().detach()
    # Initialize perturbation delta
    delta = torch.zeros_like(data_clone, requires_grad=True)
    optimizer = torch.optim.Adam([delta], lr=lr)
    
    for i in range(iterations):
        optimizer.zero_grad()
        adv_images = torch.clamp(data_clone + delta, 0, 1)
        outputs = model(adv_images)
        
        # One-hot encode target labels
        target_onehot = F.one_hot(target, num_classes=outputs.shape[1])
        # Exclude the target class by subtracting a large constant
        outputs_with_target_removed = outputs - target_onehot * 10000
        best_non_target = torch.argmax(outputs_with_target_removed, dim=1)
        
        # Calculate loss components
        target_logits = outputs.gather(1, target.unsqueeze(1)).squeeze(1)
        best_non_target_logits = outputs.gather(1, best_non_target.unsqueeze(1)).squeeze(1)
        loss_attack = torch.clamp(target_logits - best_non_target_logits + confidence, min=0)
        loss_distance = torch.norm(delta.view(delta.shape[0], -1), p=2, dim=1)
        
        loss = (c * loss_attack + loss_distance).mean()
        loss.backward()
        optimizer.step()
        
        with torch.no_grad():
            delta.data.clamp_(-epsilon, epsilon)
    
    adv_examples = torch.clamp(data_clone + delta.detach(), 0, 1)
    return adv_examples
